{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4eb1df6-6e7e-4a84-bd61-5d72091226e8",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78aff64-cba8-490f-a48e-45966b2e68b9",
   "metadata": {},
   "source": [
    "In this notebook you will begin to create chatbot functionality, creating an AI bot capable of retaining conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82975443-881e-4188-a5dc-c4f44ceb58ea",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ceca2-1a25-4ede-8cf8-11f98dcd14e6",
   "metadata": {},
   "source": [
    "### Amazon SageMaker\n",
    "Use Amazon SageMaker Jumpstart to deploy a Llama2-70B model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfcc8b39-0e40-4198-b3a9-da9b3c0018d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script uvicorn is installed in '/usr/local/python/3.10.13/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script sagemaker-upgrade-v2 is installed in '/usr/local/python/3.10.13/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066c4a0-be89-4535-a80c-4c1b66ddd6d5",
   "metadata": {},
   "source": [
    "***\n",
    "You can continue with the default model or choose a different model: this notebook will run with the following model IDs :\n",
    "- `meta-textgeneration-llama-2-7b-f`\n",
    "- `meta-textgeneration-llama-2-13b-f`\n",
    "- `meta-textgeneration-llama-2-70b-f`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7025c27f-82cb-4eee-bc10-1dad7031bf09",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-70b-f\", \"2.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0df9a6-0916-45fc-bdbd-3c7cd4b54f73",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "***\n",
    "You can now deploy the model using SageMaker JumpStart.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4b46ad-3c0d-40a8-a842-90cf1b6bbee1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/codespace/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For forward compatibility, pin to model_version='2.*' in your JumpStartModel or JumpStartEstimator definitions. Note that major version upgrades may have different EULA acceptance terms and input/output signatures.\n",
      "Using model 'meta-textgeneration-llama-2-70b-f' with wildcard version identifier '2.*'. You can pin to version '2.0.4' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "Couldn't call 'get_role' to get Role ARN from role name upgradhenryisaac to get Role path.\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateModel operation: 1 validation error detected: Value 'arn:aws:iam::114997964775:user/upgradhenryisaac' at 'executionRoleArn' failed to satisfy constraint: Member must satisfy regular expression pattern: ^arn:aws[a-z\\-]*:iam::\\d{12}:role/?[a-zA-Z_0-9+=,.@\\-_/]+$",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjumpstart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JumpStartModel\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m JumpStartModel(model_id\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m      4\u001b[0m                        model_version\u001b[38;5;241m=\u001b[39mmodel_version)\n\u001b[0;32m----> 5\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/sagemaker/jumpstart/model.py:594\u001b[0m, in \u001b[0;36mJumpStartModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, managed_instance_scaling, endpoint_type)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates endpoint by calling base ``Model`` class `deploy` method.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03mCreate a SageMaker ``Model`` and ``EndpointConfig``, and deploy an\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;124;03m        (Default: EndpointType.MODEL_BASED).\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    563\u001b[0m deploy_kwargs \u001b[38;5;241m=\u001b[39m get_deploy_kwargs(\n\u001b[1;32m    564\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[1;32m    565\u001b[0m     model_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_version,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m     endpoint_type\u001b[38;5;241m=\u001b[39mendpoint_type,\n\u001b[1;32m    592\u001b[0m )\n\u001b[0;32m--> 594\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mJumpStartModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdeploy_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_kwargs_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;66;03m# If no predictor class was passed, add defaults to predictor\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_predictor_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m async_inference_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/sagemaker/model.py:1602\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, **kwargs)\u001b[0m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# existing single model endpoint path\u001b[39;00m\n\u001b[0;32m-> 1602\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_sagemaker_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1608\u001b[0m     serverless_inference_config_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1609\u001b[0m         serverless_inference_config\u001b[38;5;241m.\u001b[39m_to_request_dict() \u001b[38;5;28;01mif\u001b[39;00m is_serverless \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1610\u001b[0m     )\n\u001b[1;32m   1611\u001b[0m     production_variant \u001b[38;5;241m=\u001b[39m sagemaker\u001b[38;5;241m.\u001b[39mproduction_variant(\n\u001b[1;32m   1612\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1613\u001b[0m         instance_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1619\u001b[0m         container_startup_health_check_timeout\u001b[38;5;241m=\u001b[39mcontainer_startup_health_check_timeout,\n\u001b[1;32m   1620\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/sagemaker/jumpstart/model.py:423\u001b[0m, in \u001b[0;36mJumpStartModel._create_sagemaker_model\u001b[0;34m(self, instance_type, accelerator_type, tags, serverless_inference_config, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     model_package\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\n\u001b[0;32m--> 423\u001b[0m \u001b[43mmodel_package\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_sagemaker_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m model_package\u001b[38;5;241m.\u001b[39m_base_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_name \u001b[38;5;241m=\u001b[39m model_package\u001b[38;5;241m.\u001b[39m_base_name\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/sagemaker/model.py:2169\u001b[0m, in \u001b[0;36mModelPackage._create_sagemaker_model\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m approval_status \u001b[38;5;241m!=\u001b[39m ModelApprovalStatusEnum\u001b[38;5;241m.\u001b[39mAPPROVED:\n\u001b[1;32m   2167\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_approval_status(approval_status\u001b[38;5;241m=\u001b[39mModelApprovalStatusEnum\u001b[38;5;241m.\u001b[39mAPPROVED)\n\u001b[0;32m-> 2169\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrole\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvpc_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvpc_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_network_isolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_network_isolation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/sagemaker/session.py:3716\u001b[0m, in \u001b[0;36mSession.create_model\u001b[0;34m(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\u001b[0m\n\u001b[1;32m   3713\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3714\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 3716\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_create_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_model_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m name\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/sagemaker/session.py:6152\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   6135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_intercept_create_request\u001b[39m(\n\u001b[1;32m   6136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   6137\u001b[0m     request: typing\u001b[38;5;241m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6140\u001b[0m     \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   6141\u001b[0m ):\n\u001b[1;32m   6142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   6143\u001b[0m \n\u001b[1;32m   6144\u001b[0m \u001b[38;5;124;03m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6150\u001b[0m \u001b[38;5;124;03m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   6151\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/sagemaker/session.py:3704\u001b[0m, in \u001b[0;36mSession.create_model.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   3702\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreateModel request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m   3703\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3704\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3705\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3706\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mresponse[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateModel operation: 1 validation error detected: Value 'arn:aws:iam::114997964775:user/upgradhenryisaac' at 'executionRoleArn' failed to satisfy constraint: Member must satisfy regular expression pattern: ^arn:aws[a-z\\-]*:iam::\\d{12}:role/?[a-zA-Z_0-9+=,.@\\-_/]+$"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model = JumpStartModel(model_id=model_id,\n",
    "                       model_version=model_version)\n",
    "predictor = model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42343e1f-40d8-4b9a-a56c-53f60774dcd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "\n",
    "import boto3, json\n",
    "sm_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "def transform_input(prompt: str, temperature) -> bytes:\n",
    "    input_str = json.dumps({\n",
    "        \"inputs\": [[{\"role\": \"user\", \"content\": prompt},]],\n",
    "        \"parameters\": {\"max_new_tokens\": 512,\"top_p\": 0.9,\"temperature\": temperature}\n",
    "    })\n",
    "    return input_str.encode('utf-8')\n",
    "    \n",
    "def transform_output(output: bytes) -> str:\n",
    "    response_json = json.loads(output['Body'].read().decode(\"utf-8\"))\n",
    "    return response_json[0][\"generation\"][\"content\"]\n",
    "\n",
    "def generate(prompt, temperature = 0.6):\n",
    "    response = sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        CustomAttributes=\"accept_eula=true\", \n",
    "        ContentType=\"application/json\",\n",
    "        Body=transform_input(prompt, temperature)\n",
    "    )\n",
    "    response_output = transform_output(response)\n",
    "    return response_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cebf00-8443-4314-be4e-0f5e876c7697",
   "metadata": {},
   "source": [
    "### Edit System Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f71874f9-15f6-445b-bd75-9753ccead4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_with_system_message(prompt, system_message):\n",
    "    prompt = f\"\"\"\n",
    "    <s>[INST] <<SYS>>{system_message}<</SYS>>\n",
    "\n",
    "    User: {prompt}\n",
    "    Agent:[/INST]\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91434430-41a6-488c-a8a5-99ffb23bc635",
   "metadata": {},
   "source": [
    "### Include One-to-many Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d6133ce-3b12-4441-8b48-9d6e2ba9896b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_with_examples(prompt, system_message, examples=[]):\n",
    "    \n",
    "    # Start with the initial part of the prompt with system message\n",
    "    full_prompt = f\"<s>[INST] <<SYS>>{system_message}<</SYS>>\\n\"\n",
    "\n",
    "    # Add each example to the prompt\n",
    "    for user_msg, agent_response in examples:\n",
    "        full_prompt += f\"{user_msg} [/INST] {agent_response} </s><s>[INST]\"\n",
    "\n",
    "    # Add the main prompt and close the template\n",
    "    full_prompt += f\"{prompt} [/INST]\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c1228-3280-4798-9558-e19139b77b6c",
   "metadata": {},
   "source": [
    "### LlamaChatbot Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36e18ccd-72e4-40aa-a2fb-d8359fb5fda8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaChatbot:\n",
    "    def __init__(self, system_message):\n",
    "        self.system_message = system_message\n",
    "        self.conversation_history = []  # list of tuples (user_msg, agent_response)\n",
    "\n",
    "    def chat(self, user_msg):\n",
    "        # Generate the prompt using the conversation history and the new user message\n",
    "        prompt = prompt_with_examples(user_msg, self.system_message, self.conversation_history)\n",
    "        \n",
    "        # Get the model's response\n",
    "        agent_response = generate(prompt)\n",
    "\n",
    "        # Store this interaction in the conversation history\n",
    "        self.conversation_history.append((user_msg, agent_response))\n",
    "\n",
    "        return agent_response\n",
    "\n",
    "    def reset(self):\n",
    "        # Clear conversation history\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d710c5b-5ac6-4f34-b0c1-e6518af71615",
   "metadata": {},
   "source": [
    "## No Conversation Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8eb9f27-a69f-4944-9fa5-062d0e7ef867",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello Rock! It's great to meet you too. Is there anything you'd like to chat about or ask? I'm here to help with any questions you might have.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello my name is Rock. Nice to meet you!\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "161570d0-5423-4270-8fdc-66f7a11d0807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm not able to access personal information, including your name. However, I can suggest ways for you to remind yourself of your name.\n",
      "\n",
      "1. Check your identification documents: Your name should be listed on your driver's license, passport, or other government-issued ID cards.\n",
      "2. Look at your social media profiles: Your name should be listed on your social media profiles, such as Facebook, Twitter, or LinkedIn.\n",
      "3. Check your email address: Your name may be included in your email address, especially if you use a personalized email address that includes your name.\n",
      "4. Ask a friend or family member: If you're having trouble remembering your name, you can ask a friend or family member to remind you.\n",
      "5. Check your personal belongings: Your name may be listed on your personal belongings, such as your phone, laptop, or other devices.\n",
      "\n",
      "I hope these suggestions help you remind yourself of your name! If you continue to have trouble remembering your name, you may want to consider speaking with a healthcare professional for further assistance.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can you remind me what my name is?\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10bd66e2-8337-4eb9-9f0a-3e49939d0247",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello Rock! It's great to meet you too! I'm just an AI, I'm here to help answer any questions you have or just chat with you if you'd like. It's nice to see you here!\n",
      "\n",
      "By the way, Rock, I'm glad you introduced yourself. I'll be sure to remember your name for next time we chat. Is there anything you'd like to talk about or ask me? I'm all ears!\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\"\n",
    "You are a friendly chatbot always eager to help and engage in meaningful conversation. \\\n",
    "You always remember the details of our previous conversations, \\\n",
    "especially if a user gives them their name.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Hello my name is Rock. Nice to meet you!\"\n",
    "\n",
    "print(generate(prompt_with_system_message(prompt, system_message)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2d0bcd6-11f5-4f33-9a8e-f657ce4a4ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure thing! Your name is [User's Name]. It's great to see you again! How can I assist you today? Is there anything new you'd like to chat about or ask for help with?\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\"\n",
    "You are a friendly chatbot always eager to help and engage in meaningful conversation. \\\n",
    "You always remember the details of our previous conversations, \\\n",
    "especially if a user gives them their name.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Can you remind me what my name is?\"\n",
    "\n",
    "print(generate(prompt_with_system_message(prompt, system_message)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb9670-9447-43bd-9f42-1c36adc9fe00",
   "metadata": {},
   "source": [
    "## Create Conversation Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdf3e428-7890-4e39-9f7c-639044c947d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaChatbot:\n",
    "    def __init__(self, system_message):\n",
    "        self.system_message = system_message\n",
    "        self.conversation_history = []  # list of tuples (user_msg, agent_response)\n",
    "\n",
    "    def chat(self, user_msg):\n",
    "        # Generate the prompt using the conversation history and the new user message\n",
    "        prompt = prompt_with_examples(user_msg, self.system_message, self.conversation_history)\n",
    "        \n",
    "        # Get the model's response\n",
    "        agent_response = generate(prompt)\n",
    "\n",
    "        # Store this interaction in the conversation history\n",
    "        self.conversation_history.append((user_msg, agent_response))\n",
    "\n",
    "        return agent_response\n",
    "\n",
    "    def reset(self):\n",
    "        # Clear conversation history\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f807d3b9-00a0-428d-beb4-07e061fcc06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are a friendly chatbot always eager to help and engage in meaningful conversation.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = LlamaChatbot(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f94b8d9d-b76a-4df8-9886-eeca66fccaff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello Rock! It's great to meet you too! I'm just an AI, my name is ChatGenie, and I'm here to help you with any questions or topics you'd like to discuss. How's your day going so far? Is there anything you'd like to chat about or ask me? I'm all ears! 😊\n"
     ]
    }
   ],
   "source": [
    "print(chatbot.chat(\"Hi, my name is Rock. Nice to meet you!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2950da7-9863-4765-8750-da1de864e4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Of course, Rock! Your name is Rock. 😊 How's your day going so far? Is there anything else you'd like to chat about or ask me? I'm here to help!\n"
     ]
    }
   ],
   "source": [
    "print(chatbot.chat(\"Can you remind me what my name is?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "729a3ad7-2d20-4cc8-8b96-95254b4e0e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "543f81f1-aefe-4334-a470-cbca4a521d64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Of course, I'd be happy to help! Unfortunately, I don't have access to personal information, so I'm unable to remind you of your name. However, I'd love to chat with you and help with any other questions you might have! Is there anything else I can assist you with? 😊\n"
     ]
    }
   ],
   "source": [
    "print(chatbot.chat(\"Can you remind me what my name is?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405bbc94-1f2b-4fe7-a28a-1b44335e3ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
